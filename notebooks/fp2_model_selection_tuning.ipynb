{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# False Positive Brand Classifier - Model Selection & Tuning\n\nThis notebook performs model selection and hyperparameter tuning for the FP classifier. It loads the feature transformer created in fp1_EDA_FE.ipynb and focuses on finding the optimal model.\n\n## Objective\nSelect the best classification model and tune hyperparameters to maximize F2 score (recall-weighted).\n\n## Contents\n1. [Setup](#setup)\n2. [Data Loading & Split](#1-data-loading--split)\n3. [Feature Transformation](#2-feature-transformation)\n4. [Baseline Model Comparison](#3-baseline-model-comparison)\n5. [Hyperparameter Tuning](#4-hyperparameter-tuning)\n6. [Overfitting Analysis](#5-overfitting-analysis)\n7. [Final Model & Threshold Selection](#6-final-model--threshold-selection)\n8. [Export for Deployment](#7-export-for-deployment)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Standard imports\nimport json\nimport sys\nimport warnings\nfrom pathlib import Path\n\nimport joblib\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\n\n# Sklearn imports\nfrom sklearn.calibration import CalibratedClassifierCV\nfrom sklearn.ensemble import HistGradientBoostingClassifier, RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import (\n    fbeta_score, make_scorer, precision_recall_curve,\n    classification_report, confusion_matrix\n)\nfrom sklearn.model_selection import GridSearchCV, StratifiedKFold\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.svm import LinearSVC\n\n# Add project root to path\nproject_root = Path.cwd().parent\nsys.path.insert(0, str(project_root))\n\n# Project imports\nfrom src.fp1_nb.data_utils import load_jsonl_data, split_train_val_test\nfrom src.fp1_nb.preprocessing import clean_text, create_text_features\nfrom src.fp1_nb.feature_transformer import FPFeatureTransformer\nfrom src.fp1_nb.modeling import (\n    create_search_object,\n    tune_with_logging,\n    get_best_params_summary,\n    compare_models,\n    evaluate_model,\n    compare_val_test_performance,\n)\n\n# Settings\nwarnings.filterwarnings('ignore')\npd.set_option('display.max_columns', 50)\nplt.style.use('seaborn-v0_8-whitegrid')\n\n# Configuration\nRANDOM_STATE = 42\nnp.random.seed(RANDOM_STATE)\nTARGET_COL = 'is_sportswear'\nN_FOLDS = 3\n\n# Paths\nDATA_PATH = project_root / 'data' / 'fp_training_data.jsonl'\nMODELS_DIR = project_root / 'models'\nIMAGES_DIR = project_root / 'images'\n\nprint(\"Setup complete!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 1. Data Loading & Split\n\nLoad data and apply identical preprocessing and split as fp1 to ensure consistency."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load the FP training data (same as fp1)\ndf = load_jsonl_data(DATA_PATH)\n\n# Create combined text features (identical to fp1)\n# This ensures we use the same preprocessing\ndf['text_features'] = create_text_features(\n    df,\n    text_col='content',\n    title_col='title',\n    brands_col='brands',\n    clean_func=clean_text\n)\n\nprint(f\"\\nText features created!\")\nprint(f\"Sample:\\n{df['text_features'].iloc[0][:300]}...\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Split with stratification (identical to fp1 - same random_state ensures identical splits)\ntrain_df, val_df, test_df = split_train_val_test(\n    df,\n    target_col=TARGET_COL,\n    train_ratio=0.6,\n    val_ratio=0.2,\n    test_ratio=0.2,\n    random_state=RANDOM_STATE\n)"
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "## 2. Feature Transformation\n\nLoad the fitted feature transformer from fp1 and transform all splits."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load the fitted feature transformer from fp1\ntransformer_path = MODELS_DIR / 'fp_feature_transformer.joblib'\ntransformer = joblib.load(transformer_path)\n\n# Load the transformer config\nconfig_path = MODELS_DIR / 'fp_feature_config.json'\nwith open(config_path) as f:\n    transformer_config = json.load(f)\n\nprint(f\"Loaded transformer: {transformer}\")\nprint(f\"\\nTransformer config:\")\nfor key, value in transformer_config.items():\n    print(f\"  {key}: {value}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Transform all splits using the fitted transformer\nX_train = transformer.transform(train_df['text_features'])\nX_val = transformer.transform(val_df['text_features'])\nX_test = transformer.transform(test_df['text_features'])\n\n# Extract targets\ny_train = train_df[TARGET_COL].values\ny_val = val_df[TARGET_COL].values\ny_test = test_df[TARGET_COL].values\n\n# Combine train+val for hyperparameter tuning\nimport scipy.sparse as sp\nif sp.issparse(X_train):\n    X_trainval = sp.vstack([X_train, X_val])\nelse:\n    X_trainval = np.vstack([X_train, X_val])\ny_trainval = np.concatenate([y_train, y_val])\n\nprint(f\"Feature dimensionality: {X_train.shape[1]}\")\nprint(f\"X_train shape: {X_train.shape}\")\nprint(f\"X_val shape: {X_val.shape}\")\nprint(f\"X_test shape: {X_test.shape}\")\nprint(f\"X_trainval shape: {X_trainval.shape} (for hyperparameter tuning)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 3. Baseline Model Comparison\n\nTrain and evaluate multiple classifiers on the transformed features to identify the best performing models for hyperparameter tuning."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cross-validation strategy\ncv = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=RANDOM_STATE)\nprint(f\"Using {N_FOLDS}-fold stratified CV\")\n\n# F2 scorer (weights recall 2x higher than precision)\nf2_scorer = make_scorer(fbeta_score, beta=2)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Define baseline models\nbaseline_models = {\n    'Logistic Regression': LogisticRegression(\n        max_iter=1000, \n        random_state=RANDOM_STATE,\n        class_weight='balanced'\n    ),\n    'Naive Bayes': MultinomialNB(),\n    'Linear SVM': CalibratedClassifierCV(\n        LinearSVC(max_iter=2000, random_state=RANDOM_STATE, class_weight='balanced'),\n        cv=3\n    ),\n    'Random Forest': RandomForestClassifier(\n        n_estimators=100,\n        random_state=RANDOM_STATE,\n        class_weight='balanced',\n        n_jobs=-1\n    ),\n}"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Train and evaluate baseline models\nbaseline_results = []\n\nfor name, model in baseline_models.items():\n    print(f\"\\nTraining {name}...\")\n    model.fit(X_train, y_train)\n    \n    # Evaluate on validation set\n    metrics = evaluate_model(\n        model, X_val, y_val,\n        model_name=name,\n        dataset_name='Validation',\n        verbose=True,\n        plot=False\n    )\n    \n    # Add F2 score to metrics (recall-weighted)\n    y_pred = model.predict(X_val)\n    metrics['f2'] = fbeta_score(y_val, y_pred, beta=2)\n    print(f\"  F2 Score:  {metrics['f2']:.4f} (recall-weighted)\")\n    \n    baseline_results.append(metrics)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Compare baseline models (with F2 as primary metric)\nbaseline_comparison = compare_models(\n    baseline_results,\n    metrics_to_display=['f2', 'recall', 'precision', 'f1', 'accuracy', 'pr_auc'],\n    title='Baseline Model Comparison (Validation Set)',\n    save_path='images/fp_baseline_comparison.png'\n)"
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "## 4. Hyperparameter Tuning\n\nTune the top-performing baseline models using cross-validation on train+val combined (80% of data). The test set remains completely held out for final evaluation."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 4.1 Logistic Regression Tuning"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Logistic Regression parameter grid\nlr_param_grid = {\n    'C': [0.01, 0.1, 1.0, 10.0],\n    'penalty': ['l1', 'l2'],\n    'solver': ['saga'],\n    'class_weight': ['balanced', None],\n}\n\nlr_search = create_search_object(\n    search_type='grid',\n    estimator=LogisticRegression(max_iter=2000, random_state=RANDOM_STATE),\n    param_grid=lr_param_grid,\n    cv=cv,\n    refit='f2'\n)\n\nlr_search, lr_log, lr_csv = tune_with_logging(\n    lr_search, X_trainval, y_trainval,\n    model_name='logistic_regression'\n)"
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "### 4.2 Random Forest Tuning"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Random Forest parameter grid\nrf_param_grid = {\n    'n_estimators': [100, 200],\n    'max_depth': [10, 20, None],\n    'min_samples_split': [2, 5],\n    'min_samples_leaf': [1, 2],\n    'class_weight': ['balanced', 'balanced_subsample'],\n}\n\nrf_search = create_search_object(\n    search_type='grid',\n    estimator=RandomForestClassifier(random_state=RANDOM_STATE, n_jobs=-1),\n    param_grid=rf_param_grid,\n    cv=cv,\n    refit='f2'\n)\n\nrf_search, rf_log, rf_csv = tune_with_logging(\n    rf_search, X_trainval, y_trainval,\n    model_name='random_forest'\n)"
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "### 4.3 HistGradientBoosting Tuning"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# HistGradientBoosting parameter grid\n# Note: Uses class_weight='balanced' for imbalanced data\nhgb_param_grid = {\n    'max_iter': [100, 200],\n    'learning_rate': [0.05, 0.1],\n    'max_depth': [3, 5, None],\n    'min_samples_leaf': [5, 20],\n    'l2_regularization': [0.0, 0.1],\n    'class_weight': ['balanced'],\n}\n\nhgb_search = create_search_object(\n    search_type='grid',\n    estimator=HistGradientBoostingClassifier(random_state=RANDOM_STATE),\n    param_grid=hgb_param_grid,\n    cv=cv,\n    refit='f2'\n)\n\nhgb_search, hgb_log, hgb_csv = tune_with_logging(\n    hgb_search, X_trainval, y_trainval,\n    model_name='hist_gradient_boosting'\n)",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "### 4.4 Compare Tuned Models (CV Performance)\n\nCompare models based on their cross-validation F2 scores. Model selection is based on CV performance, not test set performance."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Compare tuned models based on CV performance\ntuned_models = {\n    'LR_tuned': lr_search,\n    'RF_tuned': rf_search,\n    'HGB_tuned': hgb_search,\n}\n\n# Extract CV metrics for comparison\ncv_comparison_data = []\nfor name, search in tuned_models.items():\n    best_idx = search.best_index_\n    cv_results = search.cv_results_\n    \n    metrics = {\n        'model_name': name,\n        'f2': cv_results['mean_test_f2'][best_idx],\n        'recall': cv_results['mean_test_recall'][best_idx],\n        'precision': cv_results['mean_test_precision'][best_idx],\n        'f1': cv_results['mean_test_f1'][best_idx],\n        'accuracy': cv_results['mean_test_accuracy'][best_idx],\n        'pr_auc': cv_results['mean_test_average_precision'][best_idx],\n    }\n    cv_comparison_data.append(metrics)\n    \n    print(f\"{name}: CV F2 = {metrics['f2']:.4f} (+/- {cv_results['std_test_f2'][best_idx]:.4f}), Recall = {metrics['recall']:.4f}, Precision = {metrics['precision']:.4f}\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Compare tuned models (F2 as primary metric)\ntuned_comparison = compare_models(\n    cv_comparison_data,\n    metrics_to_display=['f2', 'recall', 'precision', 'f1', 'accuracy', 'pr_auc'],\n    title='Tuned Model Comparison (CV Performance, Optimized for F2)',\n    save_path='images/fp_tuned_comparison.png'\n)"
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "## 5. Overfitting Analysis\n\nAnalyze the gap between CV performance and test performance to assess model generalization."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Evaluate all tuned models on test set and compare with CV performance\nprint(\"=\" * 70)\nprint(\"OVERFITTING ANALYSIS: CV vs Test Performance\")\nprint(\"=\" * 70)\n\noverfitting_data = []\n\nfor name, search in tuned_models.items():\n    best_model = search.best_estimator_\n    best_idx = search.best_index_\n    cv_results = search.cv_results_\n    \n    # Get CV metrics\n    cv_f2 = cv_results['mean_test_f2'][best_idx]\n    cv_f2_std = cv_results['std_test_f2'][best_idx]\n    cv_recall = cv_results['mean_test_recall'][best_idx]\n    cv_precision = cv_results['mean_test_precision'][best_idx]\n    \n    # Get test metrics\n    y_pred_test = best_model.predict(X_test)\n    test_f2 = fbeta_score(y_test, y_pred_test, beta=2)\n    test_recall = (y_pred_test[y_test == 1] == 1).mean()\n    test_precision = (y_test[y_pred_test == 1] == 1).mean() if y_pred_test.sum() > 0 else 0\n    \n    # Calculate gaps\n    f2_gap = test_f2 - cv_f2\n    recall_gap = test_recall - cv_recall\n    precision_gap = test_precision - cv_precision\n    \n    print(f\"\\n{name}:\")\n    print(f\"  CV F2:       {cv_f2:.4f} (+/- {cv_f2_std:.4f})\")\n    print(f\"  Test F2:     {test_f2:.4f}\")\n    print(f\"  Gap:         {f2_gap:+.4f} {'⚠️ OVERFITTING' if f2_gap < -0.05 else '✓ OK'}\")\n    print(f\"  CV Recall:   {cv_recall:.4f} | Test Recall:   {test_recall:.4f} | Gap: {recall_gap:+.4f}\")\n    print(f\"  CV Precision:{cv_precision:.4f} | Test Precision:{test_precision:.4f} | Gap: {precision_gap:+.4f}\")\n    \n    overfitting_data.append({\n        'model': name,\n        'cv_f2': cv_f2,\n        'cv_f2_std': cv_f2_std,\n        'test_f2': test_f2,\n        'f2_gap': f2_gap,\n        'cv_recall': cv_recall,\n        'test_recall': test_recall,\n        'cv_precision': cv_precision,\n        'test_precision': test_precision,\n    })\n\nprint(\"\\n\" + \"=\" * 70)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Visualize CV vs Test performance\noverfitting_df = pd.DataFrame(overfitting_data)\n\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# F2 comparison\nax1 = axes[0]\nx = np.arange(len(overfitting_df))\nwidth = 0.35\nbars1 = ax1.bar(x - width/2, overfitting_df['cv_f2'], width, label='CV F2', color='steelblue')\nbars2 = ax1.bar(x + width/2, overfitting_df['test_f2'], width, label='Test F2', color='coral')\nax1.errorbar(x - width/2, overfitting_df['cv_f2'], yerr=overfitting_df['cv_f2_std'], fmt='none', color='black', capsize=3)\nax1.set_xlabel('Model')\nax1.set_ylabel('F2 Score')\nax1.set_title('CV vs Test F2 Score (Overfitting Check)')\nax1.set_xticks(x)\nax1.set_xticklabels(overfitting_df['model'])\nax1.legend()\nax1.set_ylim([0.9, 1.0])\nax1.axhline(y=0.95, color='gray', linestyle='--', alpha=0.5, label='Threshold')\n\n# Gap visualization\nax2 = axes[1]\ncolors = ['green' if gap >= -0.05 else 'red' for gap in overfitting_df['f2_gap']]\nbars = ax2.bar(overfitting_df['model'], overfitting_df['f2_gap'], color=colors)\nax2.axhline(y=0, color='black', linestyle='-', linewidth=0.5)\nax2.axhline(y=-0.05, color='red', linestyle='--', alpha=0.5, label='Overfitting threshold')\nax2.set_xlabel('Model')\nax2.set_ylabel('F2 Gap (Test - CV)')\nax2.set_title('F2 Gap Analysis')\nax2.legend()\n\nplt.tight_layout()\nplt.savefig('images/fp_overfitting_analysis.png', dpi=150, bbox_inches='tight')\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "## 6. Final Model & Threshold Selection\n\nSelect the best model based on CV F2 score and evaluate on the held-out test set."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Select best model based on CV F2\nbest_model_name, best_model_metrics = get_best_model(tuned_comparison, 'f2')\nprint(f\"Selected model: {best_model_name}\")\nprint(f\"CV F2: {best_model_metrics['f2']:.4f} (primary metric)\")\nprint(f\"CV Recall: {best_model_metrics['recall']:.4f}\")\nprint(f\"CV Precision: {best_model_metrics['precision']:.4f}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Get the best model and evaluate on test set\nbest_search = tuned_models[best_model_name]\nbest_model = best_search.best_estimator_\n\n# Evaluate on held-out test set (only once)\ntest_metrics = evaluate_model(\n    best_model, X_test, y_test,\n    model_name=best_model_name,\n    dataset_name='Test',\n    verbose=True,\n    plot=True,\n    save_path='images/fp_best_model_test.png'\n)\n\n# Add F2 score\ny_pred_test = best_model.predict(X_test)\ntest_metrics['f2'] = fbeta_score(y_test, y_pred_test, beta=2)\nprint(f\"  F2 Score:  {test_metrics['f2']:.4f} (recall-weighted)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 6.1 Threshold Tuning\n\nAdjust the decision threshold to optimize recall at the cost of precision."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Analyze precision-recall trade-off at different thresholds\ny_proba = best_model.predict_proba(X_test)[:, 1]\nprecisions, recalls, thresholds = precision_recall_curve(y_test, y_proba)\n\n# Find thresholds for different target recall levels\ntarget_recalls = [0.95, 0.97, 0.98, 0.99]\n\nprint(\"=\" * 70)\nprint(\"THRESHOLD ANALYSIS: Recall vs Precision Trade-off\")\nprint(\"=\" * 70)\nprint(\"\\nTarget Recall | Threshold | Actual Recall | Precision | FPs Passed\")\nprint(\"-\" * 70)\n\nfor target in target_recalls:\n    idx = np.where(recalls >= target)[0]\n    if len(idx) > 0:\n        best_idx = idx[-1]\n        threshold = thresholds[best_idx] if best_idx < len(thresholds) else 0.0\n        actual_recall = recalls[best_idx]\n        precision = precisions[best_idx]\n        \n        y_pred_custom = (y_proba >= threshold).astype(int)\n        fp_count = ((y_pred_custom == 1) & (y_test == 0)).sum()\n        total_fp = (y_test == 0).sum()\n        \n        print(f\"    {target:.0%}      |   {threshold:.3f}   |    {actual_recall:.1%}      |   {precision:.1%}    |  {fp_count}/{total_fp}\")\n\nprint(\"-\" * 70)\nprint(\"\\nLower threshold = Higher recall but more FPs pass to LLM\")\nprint(\"=\" * 70)"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Plot precision-recall curve with threshold markers\nfig, ax = plt.subplots(figsize=(10, 6))\n\nax.plot(recalls, precisions, 'b-', linewidth=2, label='Precision-Recall curve')\n\n# Mark key thresholds\nfor target in [0.95, 0.98]:\n    idx = np.where(recalls >= target)[0]\n    if len(idx) > 0:\n        best_idx = idx[-1]\n        ax.scatter(recalls[best_idx], precisions[best_idx], s=100, zorder=5,\n                  label=f'Recall >= {target:.0%} (threshold={thresholds[best_idx]:.2f})')\n\n# Mark default threshold (0.5)\ndefault_idx = np.argmin(np.abs(thresholds - 0.5))\nax.scatter(recalls[default_idx], precisions[default_idx], s=100, c='red', marker='x', zorder=5,\n          label=f'Default (threshold=0.5)')\n\nax.set_xlabel('Recall', fontsize=12)\nax.set_ylabel('Precision', fontsize=12)\nax.set_title('Precision-Recall Trade-off: Choose Threshold to Maximize Recall', fontsize=14)\nax.legend(loc='lower left')\nax.grid(True, alpha=0.3)\nax.set_xlim([0.8, 1.01])\nax.set_ylim([0.5, 1.01])\n\nplt.tight_layout()\nplt.savefig('images/fp_threshold_tradeoff.png', dpi=150, bbox_inches='tight')\nplt.show()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Set optimal threshold for deployment (targeting 98% recall)\nTARGET_RECALL = 0.98\nidx = np.where(recalls >= TARGET_RECALL)[0]\nOPTIMAL_THRESHOLD = thresholds[idx[-1]] if len(idx) > 0 else 0.5\n\nprint(\"=\" * 60)\nprint(\"OPTIMAL THRESHOLD FOR DEPLOYMENT\")\nprint(\"=\" * 60)\nprint(f\"\\nTarget recall: {TARGET_RECALL:.0%}\")\nprint(f\"Optimal threshold: {OPTIMAL_THRESHOLD:.4f}\")\n\n# Evaluate with optimal threshold\ny_pred_optimal = (y_proba >= OPTIMAL_THRESHOLD).astype(int)\noptimal_recall = (y_pred_optimal[y_test == 1] == 1).mean()\noptimal_precision = (y_test[y_pred_optimal == 1] == 1).mean()\nfp_passed = ((y_pred_optimal == 1) & (y_test == 0)).sum()\nfn_missed = ((y_pred_optimal == 0) & (y_test == 1)).sum()\n\nprint(f\"\\nWith optimal threshold on test set:\")\nprint(f\"  Recall:    {optimal_recall:.4f} ({fn_missed} sportswear articles missed)\")\nprint(f\"  Precision: {optimal_precision:.4f} ({fp_passed} false positives passed to LLM)\")\nprint(\"=\" * 60)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 7. Export for Deployment\n\nSave the complete pipeline (transformer + classifier) for Docker API deployment."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Create complete pipeline for deployment\nfull_pipeline = Pipeline([\n    ('features', transformer),\n    ('classifier', best_model)\n])\n\n# Save complete pipeline\npipeline_path = MODELS_DIR / 'fp_classifier_pipeline.joblib'\njoblib.dump(full_pipeline, pipeline_path)\nprint(f\"Complete pipeline saved to {pipeline_path}\")\n\n# Save configuration\nclassifier_config = {\n    'threshold': float(OPTIMAL_THRESHOLD),\n    'target_recall': TARGET_RECALL,\n    'transformer_method': transformer.method,\n    'classifier_type': type(best_model).__name__,\n    'cv_f2': float(best_model_metrics['f2']),\n    'test_f2': float(test_metrics['f2']),\n    'best_params': best_search.best_params_,\n}\n\nconfig_path = MODELS_DIR / 'fp_classifier_config.json'\nwith open(config_path, 'w') as f:\n    json.dump(classifier_config, f, indent=2)\nprint(f\"Configuration saved to {config_path}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Test the pipeline\nprint(\"Testing complete pipeline...\")\ntest_texts = [\n    \"Nike announces new sustainability initiative to reduce carbon emissions\",\n    \"Puma the wild cat was spotted in the mountains of Montana\"\n]\n\n# Test predict\npredictions = full_pipeline.predict(test_texts)\nprobabilities = full_pipeline.predict_proba(test_texts)[:, 1]\n\nprint(\"\\nPipeline test results:\")\nfor text, pred, prob in zip(test_texts, predictions, probabilities):\n    label = \"Sportswear\" if pred == 1 else \"False Positive\"\n    print(f\"  [{prob:.4f}] {label}: {text[:60]}...\")"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "print(\"=\" * 60)\nprint(\"FINAL RESULTS SUMMARY\")\nprint(\"=\" * 60)\nprint(f\"\\nBest Model: {best_model_name}\")\nprint(f\"Feature Method: {transformer.method}\")\nprint(f\"\\nCV Performance:\")\nprint(f\"  F2 Score:  {best_model_metrics['f2']:.4f}\")\nprint(f\"  Recall:    {best_model_metrics['recall']:.4f}\")\nprint(f\"  Precision: {best_model_metrics['precision']:.4f}\")\nprint(f\"\\nTest Set Performance:\")\nprint(f\"  F2 Score:  {test_metrics['f2']:.4f}\")\nprint(f\"  Recall:    {test_metrics['recall']:.4f}\")\nprint(f\"  Precision: {test_metrics['precision']:.4f}\")\nprint(f\"  F1 Score:  {test_metrics['f1']:.4f}\")\nprint(f\"  Accuracy:  {test_metrics['accuracy']:.4f}\")\nprint(f\"  PR-AUC:    {test_metrics['pr_auc']:.4f}\")\nprint(f\"\\nDeployment Threshold: {OPTIMAL_THRESHOLD:.4f}\")\nprint(f\"\\nSaved Artifacts:\")\nprint(f\"  - {pipeline_path}\")\nprint(f\"  - {config_path}\")\nprint(\"=\" * 60)"
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "## Next Steps\n\n1. **Deploy Model**: Integrate `fp_classifier_pipeline.joblib` into Docker API service\n2. **Monitor Performance**: Track F2/recall on new data to detect drift\n3. **Retrain Periodically**: Update model as more labeled data becomes available\n4. **Alternative FE Methods**: If performance degrades, explore sentence-transformers or doc2vec in fp1"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}