{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# False Positive Brand Classifier - Model Selection & Tuning\n",
    "\n",
    "This notebook performs model selection and hyperparameter tuning for the FP classifier. It loads the feature transformer created in fp1_EDA_FE.ipynb and focuses on finding the optimal model.\n",
    "\n",
    "## Objective\n",
    "Select the best classification model and tune hyperparameters to maximize F2 score (recall-weighted).\n",
    "\n",
    "**Important:** This notebook uses ONLY training and validation data. Test set evaluation, threshold optimization, and deployment are performed in fp3_model_evaluation_deployment.ipynb.\n",
    "\n",
    "## Contents\n",
    "1. [Setup](#setup)\n",
    "2. [Data Loading & Split](#1-data-loading--split)\n",
    "3. [Feature Transformation](#2-feature-transformation)\n",
    "4. [Baseline Model Comparison](#3-baseline-model-comparison)\n",
    "5. [Hyperparameter Tuning](#4-hyperparameter-tuning)\n",
    "6. [Overfitting Analysis](#5-overfitting-analysis)\n",
    "7. [Final Model Selection](#6-final-model-selection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup complete!\n"
     ]
    }
   ],
   "source": [
    "# Standard imports\n",
    "import json\n",
    "import sys\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.sparse as sp\n",
    "import seaborn as sns\n",
    "\n",
    "# Sklearn imports\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import fbeta_score, make_scorer\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "# Add project root to path\n",
    "project_root = Path.cwd().parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "# Project imports\n",
    "from src.fp1_nb.data_utils import load_jsonl_data, split_train_val_test\n",
    "from src.fp1_nb.preprocessing import clean_text, create_text_features\n",
    "from src.fp1_nb.feature_transformer import FPFeatureTransformer\n",
    "from src.fp1_nb.modeling import (\n",
    "    create_search_object,\n",
    "    tune_with_logging,\n",
    "    get_best_params_summary,\n",
    "    get_best_model,\n",
    "    compare_models,\n",
    "    evaluate_model,\n",
    ")\n",
    "from src.fp2_nb.overfitting_analysis import (\n",
    "    analyze_cv_train_val_gap,\n",
    "    analyze_iteration_performance,\n",
    "    get_gap_summary,\n",
    ")\n",
    "\n",
    "# Settings\n",
    "warnings.filterwarnings('ignore')\n",
    "# Suppress sklearn parallel warning (internal sklearn issue, not actionable)\n",
    "warnings.filterwarnings('ignore', message='.*sklearn.utils.parallel.delayed.*')\n",
    "pd.set_option('display.max_columns', 50)\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "\n",
    "# Configuration\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "TARGET_COL = 'is_sportswear'\n",
    "N_FOLDS = 3\n",
    "\n",
    "# Paths\n",
    "DATA_PATH = project_root / 'data' / 'fp_training_data.jsonl'\n",
    "MODELS_DIR = project_root / 'models'\n",
    "IMAGES_DIR = project_root / 'images'\n",
    "\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading & Split\n",
    "\n",
    "Load data and apply identical preprocessing and split as fp1 to ensure consistency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load the FP training data (same as fp1)\ndf = load_jsonl_data(DATA_PATH)\n\n# Create combined text features (identical to fp1)\n# This ensures we use the same preprocessing including metadata\ndf['text_features'] = create_text_features(\n    df,\n    text_col='content',\n    title_col='title',\n    brands_col='brands',\n    source_name_col='source_name',\n    category_col='category',\n    include_metadata=True,\n    clean_func=clean_text\n)\n\nprint(f\"\\nText features created!\")\nprint(f\"Sample:\\n{df['text_features'].iloc[0][:300]}...\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Split with stratification (identical to fp1 - same random_state ensures identical splits)\ntrain_df, val_df, test_df = split_train_val_test(\n    df,\n    target_col=TARGET_COL,\n    train_ratio=0.6,\n    val_ratio=0.2,\n    test_ratio=0.2,\n    random_state=RANDOM_STATE\n)\n\n# Extract metadata for discrete metadata features (same as fp1)\ntrain_source_names = train_df['source_name'].tolist()\ntrain_categories = train_df['category'].tolist()\nval_source_names = val_df['source_name'].tolist()\nval_categories = val_df['category'].tolist()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feature Transformation\n",
    "\n",
    "Load the fitted feature transformer from fp1 and transform all splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded transformer: FPFeatureTransformer(method='tfidf_word', max_features=10000, fitted=True)\n",
      "\n",
      "Transformer config:\n",
      "  method: tfidf_word\n",
      "  max_features: 10000\n",
      "  ngram_range: [1, 2]\n",
      "  min_df: 2\n",
      "  max_df: 0.95\n",
      "  sublinear_tf: True\n",
      "  norm: l2\n",
      "  char_ngram_range: [3, 5]\n",
      "  char_max_features: 5000\n",
      "  lsa_n_components: 100\n",
      "  context_window_words: 20\n",
      "  doc2vec_vector_size: 100\n",
      "  doc2vec_min_count: 2\n",
      "  doc2vec_epochs: 40\n",
      "  doc2vec_dm: 1\n",
      "  doc2vec_window: 4\n",
      "  sentence_model_name: all-MiniLM-L6-v2\n",
      "  include_vocab_features: True\n",
      "  vocab_window_size: 15\n",
      "  proximity_window_size: 10\n",
      "  random_state: 42\n"
     ]
    }
   ],
   "source": [
    "# Load the fitted feature transformer from fp1\n",
    "transformer_path = MODELS_DIR / 'fp_feature_transformer.joblib'\n",
    "transformer = joblib.load(transformer_path)\n",
    "\n",
    "# Load the transformer config\n",
    "config_path = MODELS_DIR / 'fp_feature_config.json'\n",
    "with open(config_path) as f:\n",
    "    transformer_config = json.load(f)\n",
    "\n",
    "print(f\"Loaded transformer: {transformer}\")\n",
    "print(f\"\\nTransformer config:\")\n",
    "for key, value in transformer_config.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Transform train and validation splits using the fitted transformer\n# NOTE: Test data is NOT used in this notebook - final evaluation is done in fp3\n# Pass metadata for discrete metadata features (if transformer was fitted with metadata)\nX_train = transformer.transform(\n    train_df['text_features'],\n    source_names=train_source_names,\n    categories=train_categories\n)\nX_val = transformer.transform(\n    val_df['text_features'],\n    source_names=val_source_names,\n    categories=val_categories\n)\n\n# Extract targets\ny_train = train_df[TARGET_COL].values\ny_val = val_df[TARGET_COL].values\n\n# Combine train+val for hyperparameter tuning (CV will create internal train/val splits)\nif sp.issparse(X_train):\n    X_trainval = sp.vstack([X_train, X_val])\nelse:\n    X_trainval = np.vstack([X_train, X_val])\ny_trainval = np.concatenate([y_train, y_val])\n\n# Combined metadata for hyperparameter tuning\ntrainval_source_names = train_source_names + val_source_names\ntrainval_categories = train_categories + val_categories\n\nprint(f\"Feature dimensionality: {X_train.shape[1]}\")\nprint(f\"X_train shape: {X_train.shape}\")\nprint(f\"X_val shape: {X_val.shape}\")\nprint(f\"X_trainval shape: {X_trainval.shape} (for hyperparameter tuning)\")\nprint(f\"Metadata features enabled: {transformer.include_metadata_features}\")\nprint(f\"Metadata scaler fitted: {transformer._metadata_scaler is not None}\")\nprint(f\"\\nNote: Test data reserved for final evaluation in fp3 notebook\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Baseline Model Comparison\n",
    "\n",
    "Train and evaluate multiple classifiers on the transformed features to identify the best performing models for hyperparameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 3-fold stratified CV\n"
     ]
    }
   ],
   "source": [
    "# Cross-validation strategy\n",
    "cv = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=RANDOM_STATE)\n",
    "print(f\"Using {N_FOLDS}-fold stratified CV\")\n",
    "\n",
    "# F2 scorer (weights recall 2x higher than precision)\n",
    "f2_scorer = make_scorer(fbeta_score, beta=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define baseline models\n",
    "# Note: MultinomialNB not included as it requires non-negative features\n",
    "# (sentence transformer embeddings contain negative values)\n",
    "baseline_models = {\n",
    "    'Logistic Regression': LogisticRegression(\n",
    "        max_iter=1000, \n",
    "        random_state=RANDOM_STATE,\n",
    "        class_weight='balanced'\n",
    "    ),\n",
    "    'Linear SVM': CalibratedClassifierCV(\n",
    "        LinearSVC(max_iter=2000, random_state=RANDOM_STATE, class_weight='balanced'),\n",
    "        cv=3\n",
    "    ),\n",
    "    'Random Forest': RandomForestClassifier(\n",
    "        n_estimators=100,\n",
    "        random_state=RANDOM_STATE,\n",
    "        class_weight='balanced',\n",
    "        n_jobs=-1\n",
    "    ),\n",
    "    'HistGradientBoosting': HistGradientBoostingClassifier(\n",
    "        max_iter=100,\n",
    "        max_depth=5,\n",
    "        class_weight='balanced',\n",
    "        random_state=RANDOM_STATE,\n",
    "    ),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Train and evaluate baseline models\nbaseline_results = []\n\n# Models that require dense arrays (not sparse matrices)\ndense_required_models = {'HistGradientBoosting'}\n\nfor name, model in baseline_models.items():\n    print(f\"\\nTraining {name}...\")\n    \n    # Convert to dense if required\n    if name in dense_required_models and sp.issparse(X_train):\n        X_train_fit = X_train.toarray()\n        X_val_fit = X_val.toarray()\n    else:\n        X_train_fit = X_train\n        X_val_fit = X_val\n    \n    model.fit(X_train_fit, y_train)\n    \n    # Evaluate on validation set\n    metrics = evaluate_model(\n        model, X_val_fit, y_val,\n        model_name=name,\n        dataset_name='Validation',\n        verbose=True,\n        plot=False\n    )\n    \n    # Add F2 score to metrics (recall-weighted)\n    y_pred = model.predict(X_val_fit)\n    metrics['f2'] = fbeta_score(y_val, y_pred, beta=2)\n    print(f\"  F2 Score:  {metrics['f2']:.4f} (recall-weighted)\")\n    \n    baseline_results.append(metrics)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare baseline models (with F2 as primary metric)\n",
    "baseline_comparison = compare_models(\n",
    "    baseline_results,\n",
    "    metrics_to_display=['f2', 'recall', 'precision', 'f1', 'accuracy', 'pr_auc'],\n",
    "    title='Baseline Model Comparison (Validation Set)',\n",
    "    save_path='images/fp_baseline_comparison.png'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Hyperparameter Tuning\n",
    "\n",
    "Tune the top-performing baseline models using cross-validation on train+val combined (80% of data). The test set remains completely held out for final evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Logistic Regression Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression parameter grid\n",
    "# Simplified based on prior tuning: C=0.01-0.1 with class_weight=None works best\n",
    "# L1/L2 penalty makes no difference at low C, so using l2 (simpler)\n",
    "lr_param_grid = {\n",
    "    'C': [0.01, 0.1],\n",
    "    'penalty': ['l2'],\n",
    "    'solver': ['lbfgs'],  # faster than saga for l2\n",
    "    'class_weight': [None],\n",
    "}\n",
    "\n",
    "lr_search = create_search_object(\n",
    "    search_type='grid',\n",
    "    estimator=LogisticRegression(max_iter=2000, random_state=RANDOM_STATE),\n",
    "    param_grid=lr_param_grid,\n",
    "    cv=cv,\n",
    "    refit='f2'\n",
    ")\n",
    "\n",
    "lr_search, lr_log, lr_csv = tune_with_logging(\n",
    "    lr_search, X_trainval, y_trainval,\n",
    "    model_name='logistic_regression'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Random Forest Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest parameter grid\n",
    "# Simplified based on prior tuning:\n",
    "# - n_estimators=200 dominates top runs\n",
    "# - max_depth=20/None perform similarly (removing 10)\n",
    "# - min_samples_leaf=1 dominates (removing 2)\n",
    "# - Both class_weight options work well\n",
    "rf_param_grid = {\n",
    "    'n_estimators': [200],\n",
    "    'max_depth': [20, None],\n",
    "    'min_samples_split': [2, 5],\n",
    "    'min_samples_leaf': [1],\n",
    "    'class_weight': ['balanced', 'balanced_subsample'],\n",
    "}\n",
    "\n",
    "rf_search = create_search_object(\n",
    "    search_type='grid',\n",
    "    estimator=RandomForestClassifier(random_state=RANDOM_STATE, n_jobs=-1),\n",
    "    param_grid=rf_param_grid,\n",
    "    cv=cv,\n",
    "    refit='f2'\n",
    ")\n",
    "\n",
    "rf_search, rf_log, rf_csv = tune_with_logging(\n",
    "    rf_search, X_trainval, y_trainval,\n",
    "    model_name='random_forest'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 HistGradientBoosting Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# HistGradientBoosting parameter grid\n# Simplified based on prior tuning:\n# - max_depth=5 clearly outperforms 3 and None\n# - learning_rate=0.1 dominates top runs\n# - min_samples_leaf=5 dominates (removing 20)\n# - l2_regularization=0.0 is slightly better\n# - Keeping max_iter variation to check convergence\nhgb_param_grid = {\n    'max_iter': [100, 200],\n    'learning_rate': [0.1],\n    'max_depth': [5],\n    'min_samples_leaf': [5],\n    'l2_regularization': [0.0],\n    'class_weight': ['balanced'],\n}\n\nhgb_search = create_search_object(\n    search_type='grid',\n    estimator=HistGradientBoostingClassifier(random_state=RANDOM_STATE),\n    param_grid=hgb_param_grid,\n    cv=cv,\n    refit='f2'\n)\n\n# HGB requires dense arrays - convert if sparse\nX_trainval_hgb = X_trainval.toarray() if sp.issparse(X_trainval) else X_trainval\n\nhgb_search, hgb_log, hgb_csv = tune_with_logging(\n    hgb_search, X_trainval_hgb, y_trainval,\n    model_name='hist_gradient_boosting'\n)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Compare Tuned Models (CV Performance)\n",
    "\n",
    "Compare models based on their cross-validation F2 scores. Model selection is based on CV performance, not test set performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare tuned models based on CV performance\n",
    "tuned_models = {\n",
    "    'LR_tuned': lr_search,\n",
    "    'RF_tuned': rf_search,\n",
    "    'HGB_tuned': hgb_search,\n",
    "}\n",
    "\n",
    "# Extract CV metrics for comparison\n",
    "cv_comparison_data = []\n",
    "for name, search in tuned_models.items():\n",
    "    best_idx = search.best_index_\n",
    "    cv_results = search.cv_results_\n",
    "    \n",
    "    metrics = {\n",
    "        'model_name': name,\n",
    "        'f2': cv_results['mean_test_f2'][best_idx],\n",
    "        'recall': cv_results['mean_test_recall'][best_idx],\n",
    "        'precision': cv_results['mean_test_precision'][best_idx],\n",
    "        'f1': cv_results['mean_test_f1'][best_idx],\n",
    "        'accuracy': cv_results['mean_test_accuracy'][best_idx],\n",
    "        'pr_auc': cv_results['mean_test_average_precision'][best_idx],\n",
    "    }\n",
    "    cv_comparison_data.append(metrics)\n",
    "    \n",
    "    print(f\"{name}: CV F2 = {metrics['f2']:.4f} (+/- {cv_results['std_test_f2'][best_idx]:.4f}), Recall = {metrics['recall']:.4f}, Precision = {metrics['precision']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare tuned models (F2 as primary metric)\n",
    "tuned_comparison = compare_models(\n",
    "    cv_comparison_data,\n",
    "    metrics_to_display=['f2', 'recall', 'precision', 'f1', 'accuracy', 'pr_auc'],\n",
    "    title='Tuned Model Comparison (CV Performance, Optimized for F2)',\n",
    "    save_path='images/fp_tuned_comparison.png'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Top Hyperparameter Combinations\n",
    "\n",
    "Review the top performing hyperparameter combinations for each model to guide future grid adjustments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_hyperparameter_runs(search_object, model_name, n_top=10, metric='f2'):\n",
    "    \"\"\"Extract top n hyperparameter combinations from a GridSearchCV object.\"\"\"\n",
    "    cv_results = search_object.cv_results_\n",
    "    \n",
    "    # Get parameter names (remove 'param_' prefix)\n",
    "    param_names = [k.replace('param_', '') for k in cv_results.keys() if k.startswith('param_')]\n",
    "    \n",
    "    # Build dataframe with all results\n",
    "    results_data = []\n",
    "    for i in range(len(cv_results['mean_test_' + metric])):\n",
    "        row = {\n",
    "            'rank': cv_results['rank_test_' + metric][i],\n",
    "            f'val_{metric}': cv_results['mean_test_' + metric][i],\n",
    "            f'val_{metric}_std': cv_results['std_test_' + metric][i],\n",
    "            f'train_{metric}': cv_results['mean_train_' + metric][i],\n",
    "            'gap': cv_results['mean_train_' + metric][i] - cv_results['mean_test_' + metric][i],\n",
    "        }\n",
    "        # Add parameter values\n",
    "        for param in param_names:\n",
    "            row[param] = cv_results[f'param_{param}'][i]\n",
    "        results_data.append(row)\n",
    "    \n",
    "    df = pd.DataFrame(results_data)\n",
    "    df = df.sort_values('rank').head(n_top)\n",
    "    \n",
    "    return df, param_names\n",
    "\n",
    "# Display top 10 hyperparameter combinations for each model\n",
    "for model_name, search in tuned_models.items():\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"TOP 10 HYPERPARAMETER COMBINATIONS: {model_name}\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    top_df, param_names = get_top_hyperparameter_runs(search, model_name, n_top=10, metric='f2')\n",
    "    \n",
    "    # Display with formatted output\n",
    "    display_cols = ['rank', 'val_f2', 'val_f2_std', 'train_f2', 'gap'] + param_names\n",
    "    print(top_df[display_cols].to_string(index=False))\n",
    "    print()\n",
    "    \n",
    "    # Also show best parameters explicitly\n",
    "    print(f\"Best parameters: {search.best_params_}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Overfitting Analysis\n",
    "\n",
    "Analyze the gap between training and validation performance using CV results. This analysis uses only training data (through cross-validation) and does NOT use the held-out test set.\n",
    "\n",
    "**Key insight**: In GridSearchCV, `mean_train_*` scores are computed on the training folds and `mean_test_*` scores are computed on the validation folds. A large gap indicates overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get summary of train-validation gaps for all tuned models\n",
    "gap_summary_df = get_gap_summary(tuned_models, metric='f2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed train-validation gap analysis for Random Forest\n",
    "# Shows diagnosis (Good fit / Moderate / Severe overfitting) and recommendations\n",
    "rf_gap_result = analyze_cv_train_val_gap(\n",
    "    rf_search, \n",
    "    metric='f2',\n",
    "    model_name='Random Forest',\n",
    "    save_path='images/fp_rf_train_val_gap.png'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 RF Performance vs n_estimators\n",
    "\n",
    "Analyze how Random Forest performance changes with the number of trees, using optimal parameters from tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a grid varying only n_estimators while keeping other RF params at optimal values\n",
    "best_rf_params = rf_search.best_params_.copy()\n",
    "n_estimators_values = [50, 100, 150, 200, 250, 300]\n",
    "\n",
    "# Build param grid with all params fixed except n_estimators\n",
    "rf_iteration_param_grid = {\n",
    "    'n_estimators': n_estimators_values,\n",
    "    'max_depth': [best_rf_params.get('max_depth')],\n",
    "    'min_samples_split': [best_rf_params.get('min_samples_split', 2)],\n",
    "    'min_samples_leaf': [best_rf_params.get('min_samples_leaf', 1)],\n",
    "    'class_weight': [best_rf_params.get('class_weight', 'balanced')],\n",
    "}\n",
    "\n",
    "# Run grid search with this restricted grid\n",
    "rf_iteration_search = create_search_object(\n",
    "    search_type='grid',\n",
    "    estimator=RandomForestClassifier(random_state=RANDOM_STATE, n_jobs=-1),\n",
    "    param_grid=rf_iteration_param_grid,\n",
    "    cv=cv,\n",
    "    refit='f2'\n",
    ")\n",
    "\n",
    "rf_iteration_search, _, _ = tune_with_logging(\n",
    "    rf_iteration_search, X_trainval, y_trainval,\n",
    "    model_name='rf_iterations'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze performance across n_estimators values with confidence bands\n",
    "# Shows train/val scores with gap annotation at optimal point\n",
    "iteration_result = analyze_iteration_performance(\n",
    "    rf_iteration_search,\n",
    "    param_name='n_estimators',\n",
    "    metric='f2',\n",
    "    tuned_value=best_rf_params.get('n_estimators', 200),\n",
    "    model_name='Random Forest',\n",
    "    save_path='images/fp_rf_iteration_performance.png'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Final Model Selection\n",
    "\n",
    "Select the best model based on CV F2 score and save artifacts for final evaluation in fp3.\n",
    "\n",
    "**Note:** Test set evaluation, threshold optimization, and deployment are performed in fp3_model_evaluation_deployment.ipynb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select best model based on CV F2\n",
    "best_model_name, best_model_metrics = get_best_model(tuned_comparison, 'f2')\n",
    "print(f\"Selected model: {best_model_name}\")\n",
    "print(f\"CV F2: {best_model_metrics['f2']:.4f} (primary metric)\")\n",
    "print(f\"CV Recall: {best_model_metrics['recall']:.4f}\")\n",
    "print(f\"CV Precision: {best_model_metrics['precision']:.4f}\")\n",
    "\n",
    "# Get the best fitted model\n",
    "best_search = tuned_models[best_model_name]\n",
    "best_model = best_search.best_estimator_\n",
    "\n",
    "# Save best classifier for fp3\n",
    "classifier_path = MODELS_DIR / 'fp_best_classifier.joblib'\n",
    "joblib.dump(best_model, classifier_path)\n",
    "print(f\"\\nBest classifier saved to: {classifier_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save CV metrics for comparison in fp3\n",
    "cv_metrics = {\n",
    "    'model_name': best_model_name,\n",
    "    'cv_f2': float(best_model_metrics['f2']),\n",
    "    'cv_recall': float(best_model_metrics['recall']),\n",
    "    'cv_precision': float(best_model_metrics['precision']),\n",
    "    'cv_f1': float(best_model_metrics['f1']),\n",
    "    'cv_accuracy': float(best_model_metrics['accuracy']),\n",
    "    'cv_pr_auc': float(best_model_metrics['pr_auc']),\n",
    "    'best_params': best_search.best_params_,\n",
    "    'transformer_method': transformer.method,\n",
    "    'n_folds': N_FOLDS,\n",
    "}\n",
    "\n",
    "cv_metrics_path = MODELS_DIR / 'fp_cv_metrics.json'\n",
    "with open(cv_metrics_path, 'w') as f:\n",
    "    json.dump(cv_metrics, f, indent=2)\n",
    "print(f\"CV metrics saved to: {cv_metrics_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"FP2 NOTEBOOK SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nBest Model: {best_model_name}\")\n",
    "print(f\"Feature Method: {transformer.method}\")\n",
    "print(f\"\\nCV Performance ({N_FOLDS}-fold):\")\n",
    "print(f\"  F2 Score:  {best_model_metrics['f2']:.4f}\")\n",
    "print(f\"  Recall:    {best_model_metrics['recall']:.4f}\")\n",
    "print(f\"  Precision: {best_model_metrics['precision']:.4f}\")\n",
    "print(f\"\\nBest Hyperparameters:\")\n",
    "for param, value in best_search.best_params_.items():\n",
    "    print(f\"  {param}: {value}\")\n",
    "print(f\"\\nSaved Artifacts for fp3:\")\n",
    "print(f\"  - {classifier_path}\")\n",
    "print(f\"  - {cv_metrics_path}\")\n",
    "print(f\"\\nNext: Run fp3_model_evaluation_deployment.ipynb for:\")\n",
    "print(f\"  - Final test set evaluation\")\n",
    "print(f\"  - Threshold optimization\")\n",
    "print(f\"  - Pipeline export for deployment\")\n",
    "print(\"=\" * 60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}