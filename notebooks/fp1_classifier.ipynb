{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# False Positive Brand Classifier\n",
    "\n",
    "This notebook develops a classifier to detect whether an article mentioning a brand name is actually about the sportswear company or something else (e.g., \"Puma\" the animal, \"Patagonia\" the region).\n",
    "\n",
    "## Objective\n",
    "Replace Claude's false positive detection with a cost-efficient ML classifier that can filter articles before expensive LLM labeling.\n",
    "\n",
    "## Contents\n",
    "1. [Data Loading & Exploration](#1-data-loading--exploration)\n",
    "2. [Target Variable Analysis](#2-target-variable-analysis)\n",
    "3. [Text Analysis & EDA](#3-text-analysis--eda)\n",
    "4. [Data Preprocessing](#4-data-preprocessing)\n",
    "5. [Train/Validation/Test Split](#5-trainvalidationtest-split)\n",
    "6. [Baseline Models](#6-baseline-models)\n",
    "7. [Hyperparameter Tuning](#7-hyperparameter-tuning)\n",
    "8. [Model Selection & Final Evaluation](#8-model-selection--final-evaluation)\n",
    "9. [Conclusions](#9-conclusions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import sys\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "# Sklearn imports\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "\n",
    "# Add project root to path\n",
    "project_root = Path.cwd().parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "# Project imports\n",
    "from src.fp1_nb.data_utils import (\n",
    "    load_jsonl_data,\n",
    "    analyze_target_stats,\n",
    "    plot_target_distribution,\n",
    "    split_train_val_test,\n",
    ")\n",
    "from src.fp1_nb.eda_utils import (\n",
    "    analyze_text_length_stats,\n",
    "    plot_text_length_distributions,\n",
    "    analyze_brand_distribution,\n",
    "    plot_brand_distribution,\n",
    "    analyze_word_frequencies,\n",
    ")\n",
    "from src.fp1_nb.preprocessing import (\n",
    "    clean_text,\n",
    "    create_text_features,\n",
    "    build_tfidf_pipeline,\n",
    ")\n",
    "from src.fp1_nb.modeling import (\n",
    "    create_search_object,\n",
    "    tune_with_logging,\n",
    "    get_best_params_summary,\n",
    "    compare_models,\n",
    "    get_best_model,\n",
    "    evaluate_model,\n",
    "    compare_val_test_performance,\n",
    ")\n",
    "\n",
    "# Settings\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_columns', 50)\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "\n",
    "# Random seed for reproducibility\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading & Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the FP training data\n",
    "DATA_PATH = project_root / 'data' / 'fp_training_data.jsonl'\n",
    "df = load_jsonl_data(DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview the data\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data types and info\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"Missing values:\")\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine the source distribution\n",
    "print(\"\\nSource distribution:\")\n",
    "print(df['source'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Target Variable Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define target column and labels\n",
    "TARGET_COL = 'is_sportswear'\n",
    "LABEL_NAMES = ['Not Sportswear (FP)', 'Sportswear']\n",
    "\n",
    "# Analyze target distribution\n",
    "target_stats = analyze_target_stats(\n",
    "    df, \n",
    "    TARGET_COL, \n",
    "    label_names=LABEL_NAMES,\n",
    "    imbalance_threshold=5.0,\n",
    "    save_path='images/fp_target_distribution.png'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Text Analysis & EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Text Length Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze text lengths\n",
    "text_stats = analyze_text_length_stats(df, 'content', TARGET_COL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot text length distributions by class\n",
    "plot_text_length_distributions(\n",
    "    df, 'content', TARGET_COL,\n",
    "    label_names={0: 'Not Sportswear', 1: 'Sportswear'},\n",
    "    save_path='images/fp_text_length_dist.png'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Brand Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze brand distribution\n",
    "brand_counts = analyze_brand_distribution(df, 'brands', TARGET_COL, top_n=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot brand distribution by class\n",
    "plot_brand_distribution(\n",
    "    df, 'brands', TARGET_COL,\n",
    "    label_names={0: 'Not Sportswear', 1: 'Sportswear'},\n",
    "    top_n=12,\n",
    "    figsize=(14, 5),\n",
    "    save_path='images/fp_brand_dist.png'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Word Frequency Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze word frequencies by class\n",
    "word_freqs = analyze_word_frequencies(df, 'content', TARGET_COL, top_n=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at some false positive examples\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SAMPLE FALSE POSITIVE ARTICLES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "fp_samples = df[df[TARGET_COL] == 0].sample(3, random_state=RANDOM_STATE)\n",
    "for _, row in fp_samples.iterrows():\n",
    "    print(f\"\\nBrands: {row['brands']}\")\n",
    "    print(f\"Title: {row['title'][:100]}...\")\n",
    "    if 'fp_reason' in row and pd.notna(row['fp_reason']):\n",
    "        print(f\"Reason: {row['fp_reason']}\")\n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create combined text features\n",
    "# Include title (weighted), brands, and content\n",
    "df['text_features'] = create_text_features(\n",
    "    df,\n",
    "    text_col='content',\n",
    "    title_col='title',\n",
    "    brands_col='brands',\n",
    "    clean_func=clean_text\n",
    ")\n",
    "\n",
    "print(\"Text features created!\")\n",
    "print(f\"Sample:\\n{df['text_features'].iloc[0][:500]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for empty text features\n",
    "empty_texts = (df['text_features'].str.len() == 0).sum()\n",
    "print(f\"Records with empty text features: {empty_texts}\")\n",
    "\n",
    "# Remove if any\n",
    "if empty_texts > 0:\n",
    "    df = df[df['text_features'].str.len() > 0].copy()\n",
    "    print(f\"Remaining records: {len(df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Train/Validation/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split with stratification\n",
    "train_df, val_df, test_df = split_train_val_test(\n",
    "    df,\n",
    "    target_col=TARGET_COL,\n",
    "    train_ratio=0.6,\n",
    "    val_ratio=0.2,\n",
    "    test_ratio=0.2,\n",
    "    random_state=RANDOM_STATE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare feature and target arrays\n",
    "X_train = train_df['text_features']\n",
    "y_train = train_df[TARGET_COL]\n",
    "\n",
    "X_val = val_df['text_features']\n",
    "y_val = val_df[TARGET_COL]\n",
    "\n",
    "X_test = test_df['text_features']\n",
    "y_test = test_df[TARGET_COL]\n",
    "\n",
    "print(f\"X_train shape: {len(X_train)}\")\n",
    "print(f\"X_val shape: {len(X_val)}\")\n",
    "print(f\"X_test shape: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Baseline Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build TF-IDF vectorizer (fit on training data only)\n",
    "tfidf_pipeline = build_tfidf_pipeline(\n",
    "    max_features=10000,\n",
    "    ngram_range=(1, 2),\n",
    "    min_df=2,\n",
    "    max_df=0.95\n",
    ")\n",
    "\n",
    "# Fit and transform\n",
    "X_train_tfidf = tfidf_pipeline.fit_transform(X_train)\n",
    "X_val_tfidf = tfidf_pipeline.transform(X_val)\n",
    "X_test_tfidf = tfidf_pipeline.transform(X_test)\n",
    "\n",
    "print(f\"TF-IDF features: {X_train_tfidf.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define baseline models\n",
    "baseline_models = {\n",
    "    'Logistic Regression': LogisticRegression(\n",
    "        max_iter=1000, \n",
    "        random_state=RANDOM_STATE,\n",
    "        class_weight='balanced'\n",
    "    ),\n",
    "    'Naive Bayes': MultinomialNB(),\n",
    "    'Linear SVM': CalibratedClassifierCV(\n",
    "        LinearSVC(max_iter=2000, random_state=RANDOM_STATE, class_weight='balanced'),\n",
    "        cv=3\n",
    "    ),\n",
    "    'Random Forest': RandomForestClassifier(\n",
    "        n_estimators=100,\n",
    "        random_state=RANDOM_STATE,\n",
    "        class_weight='balanced',\n",
    "        n_jobs=-1\n",
    "    ),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and evaluate baseline models\n",
    "baseline_results = []\n",
    "\n",
    "for name, model in baseline_models.items():\n",
    "    print(f\"\\nTraining {name}...\")\n",
    "    model.fit(X_train_tfidf, y_train)\n",
    "    \n",
    "    # Evaluate on validation set\n",
    "    metrics = evaluate_model(\n",
    "        model, X_val_tfidf, y_val,\n",
    "        model_name=name,\n",
    "        dataset_name='Validation',\n",
    "        verbose=True,\n",
    "        plot=False\n",
    "    )\n",
    "    baseline_results.append(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare baseline models\n",
    "baseline_comparison = compare_models(\n",
    "    baseline_results,\n",
    "    metrics_to_display=['accuracy', 'precision', 'recall', 'f1', 'roc_auc', 'pr_auc'],\n",
    "    title='Baseline Model Comparison (Validation Set)',\n",
    "    save_path='images/fp_baseline_comparison.png'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-validation strategy\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1 Logistic Regression Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression parameter grid\n",
    "lr_param_grid = {\n",
    "    'C': [0.01, 0.1, 1.0, 10.0],\n",
    "    'penalty': ['l1', 'l2'],\n",
    "    'solver': ['saga'],\n",
    "    'class_weight': ['balanced', None],\n",
    "}\n",
    "\n",
    "lr_search = create_search_object(\n",
    "    search_type='grid',\n",
    "    estimator=LogisticRegression(max_iter=2000, random_state=RANDOM_STATE),\n",
    "    param_grid=lr_param_grid,\n",
    "    cv=cv,\n",
    "    refit='average_precision'\n",
    ")\n",
    "\n",
    "lr_search, lr_log, lr_csv = tune_with_logging(\n",
    "    lr_search, X_train_tfidf, y_train,\n",
    "    model_name='logistic_regression'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get best LR parameters\n",
    "lr_summary = get_best_params_summary(lr_search, 'Logistic Regression')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Random Forest Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest parameter grid\n",
    "rf_param_grid = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'max_depth': [10, 20, None],\n",
    "    'min_samples_split': [2, 5],\n",
    "    'min_samples_leaf': [1, 2],\n",
    "    'class_weight': ['balanced', 'balanced_subsample'],\n",
    "}\n",
    "\n",
    "rf_search = create_search_object(\n",
    "    search_type='grid',\n",
    "    estimator=RandomForestClassifier(random_state=RANDOM_STATE, n_jobs=-1),\n",
    "    param_grid=rf_param_grid,\n",
    "    cv=cv,\n",
    "    refit='average_precision'\n",
    ")\n",
    "\n",
    "rf_search, rf_log, rf_csv = tune_with_logging(\n",
    "    rf_search, X_train_tfidf, y_train,\n",
    "    model_name='random_forest'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get best RF parameters\n",
    "rf_summary = get_best_params_summary(rf_search, 'Random Forest')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3 Gradient Boosting Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient Boosting parameter grid (smaller for speed)\n",
    "gb_param_grid = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'learning_rate': [0.05, 0.1],\n",
    "    'max_depth': [3, 5],\n",
    "    'min_samples_split': [2, 5],\n",
    "    'subsample': [0.8, 1.0],\n",
    "}\n",
    "\n",
    "gb_search = create_search_object(\n",
    "    search_type='grid',\n",
    "    estimator=GradientBoostingClassifier(random_state=RANDOM_STATE),\n",
    "    param_grid=gb_param_grid,\n",
    "    cv=cv,\n",
    "    refit='average_precision'\n",
    ")\n",
    "\n",
    "gb_search, gb_log, gb_csv = tune_with_logging(\n",
    "    gb_search, X_train_tfidf, y_train,\n",
    "    model_name='gradient_boosting'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get best GB parameters\n",
    "gb_summary = get_best_params_summary(gb_search, 'Gradient Boosting')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.4 Compare Tuned Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate tuned models on validation set\n",
    "tuned_models = {\n",
    "    'LR_tuned': lr_search.best_estimator_,\n",
    "    'RF_tuned': rf_search.best_estimator_,\n",
    "    'GB_tuned': gb_search.best_estimator_,\n",
    "}\n",
    "\n",
    "tuned_results = []\n",
    "for name, model in tuned_models.items():\n",
    "    metrics = evaluate_model(\n",
    "        model, X_val_tfidf, y_val,\n",
    "        model_name=name,\n",
    "        dataset_name='Validation',\n",
    "        verbose=False,\n",
    "        plot=False\n",
    "    )\n",
    "    tuned_results.append(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare tuned models\n",
    "tuned_comparison = compare_models(\n",
    "    tuned_results,\n",
    "    metrics_to_display=['accuracy', 'precision', 'recall', 'f1', 'roc_auc', 'pr_auc'],\n",
    "    title='Tuned Model Comparison (Validation Set)',\n",
    "    save_path='images/fp_tuned_comparison.png'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Model Selection & Final Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select best model based on PR-AUC (important for imbalanced data)\n",
    "best_model_name, best_model_metrics = get_best_model(tuned_comparison, 'pr_auc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best model\n",
    "best_model = tuned_models[best_model_name]\n",
    "\n",
    "# Final evaluation on validation set (with plots)\n",
    "val_metrics = evaluate_model(\n",
    "    best_model, X_val_tfidf, y_val,\n",
    "    model_name=best_model_name,\n",
    "    dataset_name='Validation',\n",
    "    verbose=True,\n",
    "    plot=True,\n",
    "    save_path='images/fp_best_model_validation.png'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final evaluation on TEST set\n",
    "test_metrics = evaluate_model(\n",
    "    best_model, X_test_tfidf, y_test,\n",
    "    model_name=best_model_name,\n",
    "    dataset_name='Test',\n",
    "    verbose=True,\n",
    "    plot=True,\n",
    "    save_path='images/fp_best_model_test.png'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare validation vs test performance\n",
    "generalization_check = compare_val_test_performance(val_metrics, test_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"FINAL RESULTS SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nBest Model: {best_model_name}\")\n",
    "print(f\"\\nTest Set Performance:\")\n",
    "print(f\"  Accuracy:  {test_metrics['accuracy']:.4f}\")\n",
    "print(f\"  Precision: {test_metrics['precision']:.4f}\")\n",
    "print(f\"  Recall:    {test_metrics['recall']:.4f}\")\n",
    "print(f\"  F1 Score:  {test_metrics['f1']:.4f}\")\n",
    "print(f\"  ROC-AUC:   {test_metrics['roc_auc']:.4f}\")\n",
    "print(f\"  PR-AUC:    {test_metrics['pr_auc']:.4f}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the best model and TF-IDF vectorizer for deployment\n",
    "import joblib\n",
    "\n",
    "models_dir = project_root / 'models'\n",
    "models_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Save model\n",
    "model_path = models_dir / 'fp_classifier_model.joblib'\n",
    "joblib.dump(best_model, model_path)\n",
    "print(f\"Model saved to {model_path}\")\n",
    "\n",
    "# Save TF-IDF vectorizer\n",
    "tfidf_path = models_dir / 'fp_classifier_tfidf.joblib'\n",
    "joblib.dump(tfidf_pipeline, tfidf_path)\n",
    "print(f\"TF-IDF vectorizer saved to {tfidf_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next Steps\n",
    "\n",
    "1. **Deploy Model**: Integrate the classifier into the labeling pipeline to filter false positives before Claude labeling\n",
    "2. **Monitor Performance**: Track precision/recall on new data to detect drift\n",
    "3. **Retrain Periodically**: Update model as more labeled data becomes available\n",
    "4. **Consider Deep Learning**: If more data is collected, try transformer-based models (DistilBERT)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
